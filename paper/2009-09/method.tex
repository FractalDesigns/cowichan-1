\section{Methodology\label{s:method}}

The main aim of this work is to assess and compare
the ease with which programs can be developed and/or ported
using different parallel programming systems.
Here,
we discuss issues related to choosing the problems that make up the suite,
assessing the complexity of particular implementations,
and other purposes which implementations of these might serve.
Recognizing that these problems are extremely simple,
and easy to parallelize,
we refer to them as ``toys''.

\subsection{Criteria for Selection\label{s:method-criteria}}

Our criteria for including problems in this suite are as follows:
\begin{enumerate}
\item	Each toy should require no more than an afternoon
	to write and test in a well-supported sequential language.
	We feel that making individual problems more complicated will only discourage implementation.
\item	The correctness of each implementation must be relatively easy to verify.
	Toys whose output is easily visualized are therefore to be preferred,
	as are toys whose results are insensitive to floating-point discretization effects.
\item	Each toy application should be amenable to
	an analysis of speedup or isoefficiency \cite{b:isoefficiency-tutorial}
	to allow a comparison of theory and practice.
\item	At least some toys should not be ``infinitely scalable''.
	Many real-world applications are not,
	and this suite should reflect such limitations.
\item	At least some toys should require I/O,
	since this important aspect of real-world programming is often neglected by PPS designers.
\item	There should be some potential redundancy in the type of parallelization done,
	so that re-use of software can be demonstrated.
\item	Together,
	the toys in the suite must exercise a wide range of common parallel operations
	(Appendix~\ref{s:cliche-ops})
	and memory reference patterns
	(Appendix~\ref{s:cliche-memref}).
\item	The ``obvious'' implementations of these toys should span
	a wide range of parallel programming paradigms,
	such as task farming,
	geometric and functional decomposition,
	and speculative parallelism.
\item	Parallelizations for several (if not all) of
	the types of programming systems shown in Figure~\ref{f:taxonomy} should be known.
	This suite is not intended to be a collection of research problems;
	reasonably simple and efficient implementations in most environments
	should be straightforward to produce.
\end{enumerate}

\begin{figure}
% \epsfysize=0.5\textheight
% \begin{center}\mbox{\epsffile{fig/taxonomy.eps}}\end{center}
\caption{A Taxonomy of Parallel Programming Systems\label{f:taxonomy}}
\end{figure}

\subsection{Specification\label{s:method-spec}}

Toys should be specified in terms of inputs and outputs,
rather than algorithmically.
For example,
``sort $N$ integers in the range $1{\ldots}R$,
where $R$ is not known in advance and there may be multiple instances of a given value''
is to be preferred over
``parallelize quicksort''.
As \cite{b:slalom} argued,
which parallel algorithm is best in a particular programming system
depends critically on the architectural assumptions made by that system.
Implementors should be free to employ those methods which they find natural and convenient.
The only exception we make to this rule is in the two matrix equation solvers,
where we require Gaussian elimination and successive over-relaxation.
Given the enormous number of matrix solution algorithms in use,
we felt we had to specify these particular algorithms in order to ensure that
different implementations of this suite would be comparable.

\subsection{Software Engineering Issues\label{s:method-softeng}}

These toys will exercise many different aspects of parallel systems.
However,
their ``single algorithm per program'' model is not representative of real applications.
These usually contain several discrete (and sometimes overlapping) phases,
each of which is qualitatively different.
A full implementation of this suite will therefore have two parts.
In the first,
each toy will be implemented as a stand-alone program.
In the second,
toys will be chained together to create the kind of phase behaviour seen in real applications
(Figure~\ref{f:chaining}).
This will test the ease with which heterogeneous parallelism can be mixed within a single program.
It will also show how well the system supports code re-use and information hiding,
which are crucial to the development of large programs.

\begin{figure}
% \epsfxsize=12cm
% \begin{center}\mbox{\epsffile{fig/chain.eps}}\end{center}
\caption{Chaining\label{f:chaining}}
\end{figure}

A second reason for chaining toys is that
many groups are now using algorithmic skeletons to structure parallel programs
\cite{b:cole-skel,b:p3l-overview,b:enterprise}.
Such skeletons encapsulate the details of a particular style of parallelism,
and hide low-level or platform-dependent issues.
We hope that chaining will show the strengths and weaknesses of such systems.

Finally,
chaining should be designed so that some toys can be executed concurrently.
Many PPSs impose extraneous constraints on programs,
e.g.\ require all processes to participate in every barrier synchronization,
or require the same executable to be loaded onto each processor.
These constraints can limit the exploitation of potential concurrency.
Permitting, but not requiring, concurrent execution of several toys should uncover such limitations.

\subsection{Sizing\label{s:method-size}}

One crucial aspect of the specification of toys is
the way in which the sizes of problems are determined.
In a completely frozen model, the actual size of each problem would be compiled into each toy.
A fluid implementation, by contrast, would allow sizes to be specified at run-time,
and would only then allocate and partition data structures.
We have chosen to use an intermediate model (which might be called ``slushy''),
in which the maximum size of individual problems is specified during compilation,
but the actual size of a problem is only determined when the toy begins to execute.
We hope that the use of this model will force implementors to deal with
at least some of the software engineering issues
involved in building flexible large-scale applications,
without making implementation prohibitively difficult.

\subsection{Assessing Complexity\label{s:method-complex}}

There are at least two different ways in which usability might be compared.
One would be to measure the performance achieved by an ``average'' programmer
as a function of time on each of a range of problems.
This would lead to graphs of the kind shown in Figure~\ref{f:perf},
where the left graph shows a particular result
and the right graph a simplified abstraction of it.
By analogy with Hockney's $n_{1/2}$ measure \cite{b:hockney-perfparam},
which is the length of vector on which a pipelined architecture achieves
half its theoretical peak performance,
we could in principle find the value of $p_{1/2}$---the programming time required to achieve
half of a machine's peak performance\footnote{Note that
	if performance was measured as a fraction of the figures quoted by manufacturers for their machines,
	it is unlikely that the halfway mark would ever be reached.}
for a particular combination of programming system and problem type.

\begin{figure}
% \epsfxsize=12cm
% \begin{center}\mbox{\epsffile{fig/perf-time.eps}}\end{center}
\caption{Performance vs.\ Time\label{f:perf}}
\end{figure}

A second option would be to measure the complexities of
implementations of a single application using different programming systems.
With this approach,
comparative performance or speedup figures would be supplemented by complexity measures,
as shown in Figure~\ref{f:complex}.

\begin{figure}
% \epsfxsize=6cm
% \begin{center}\mbox{\epsffile{fig/perf-complex.eps}}\end{center}
\caption{Comparative Code Complexity\label{f:complex}}
\end{figure}

The drawback of the first approach is that there are no ``typical'' problems,
or ``average'' programmers.
Meaningful measurement of  $p_{1/2}$ would therefore require
measurement of a large number of programmers working on many problems.
Since this is too expensive to be practical,
the second approach is the one we have chosen to adopt.
It has drawbacks as well,
most notably the fact that no-one has ever demonstrated any better measure of program complexity
than a simple count of the number of lines in a program \cite{b:fenton-metrics}.
Since we anticipate a wide variation in the types of progamming systems that will be used in this work
(ranging from declarative dataflow languages
through functional languages with Lisp-like syntax
to parallel dialects of Fortran and C),
and in the indentation, commenting, and layout styles of particular programmers,
we may measure complexity by counting the number of tokens in the source code which are
keywords,
intrinsic operators,
user-defined functions,
type or variable declarations,
and miscellaneous punctuation.

\subsection{Other Uses for Implementations\label{s:method-uses}}

This suite is intended for assessing PPS usability,
but we envisage at least three other uses for implementations of it:
\begin{description}
\item[Related Research:]
	We hope that the existence of this suite will indicate
	to the developers of new parallel programming systems
	what a mature tool should be able to support.
	If sufficiently diverse,
	the suite could also be used to help test the correctness of new or improved systems.
	Finally,
	this suite could complement existing performance benchmarks,
	since performance figures for individual toys could be compared.
\item[Tool Development:]
	Requests from tool developers for existing parallel programs,
	or for event traces from them,
	are common on newsgroups such as {\tt{comp.parallel}}.
	We hope that traces from implementations of this suite
	will be used to compare different debugging and performance monitoring tools,
	that their source code will be used to test
	new parallelizing compilation techniques or mapping and scheduling algorithms,
	and so on.
\item[Teaching:]
	Implementations will be suitable for use as classroom examples,
	since they will be small enough to be understood quickly.
	The toys themselves should also be suitable as classroom exercises
	in a senior undergraduate course on parallel computing.
\end{description}

\subsection{Some Criticisms\label{s:method-criticism}}

\noindent {\em{Why not use an existing benchmark suite for this purpose?}\/}

Implementations of benchmark suites have usually been coded for absolute speed.
They are not representative of ``normal'' practice,
in which reasonable performance is the usual goal,
and so are inappropriate for this work.
In addition,
most existing benchmark suites are too large to be implemented or ported quickly
(e.g.\ SPEC~\cite{b:bench-over}),
over-specified (e.g.\ the Livermore Loops~\cite{b:livermore-loops}),
or concerned with a limited range of applications
(e.g.\ the NAS Parallel Benchmarks~\cite{b:nas-results}).
Finally,
most exist only in C or Fortran (or both);
many interesting parallel programming systems are built on top of other languages,
including Modula-2, Scheme, Prolog, and various dataflow languages.
We do not want the effort required to translate several large programs
to discourage researchers from participating in this exercise.

\vspace{\baselineskip}

\noindent {\em{Won't the results depend primarily on programmer ability?}\/}

The short answer to this question is yes,
but no more than the results from performance benchmarks.
A longer answer is that	since we intend to measure the combination of code complexity and performance achieved,
rather than performance as a function of development time,
programmers will be allowed to revise and improve their implementations when and as desired.
