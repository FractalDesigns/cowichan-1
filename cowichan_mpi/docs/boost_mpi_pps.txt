1. History and Philosophy

(a) For whom and for what purpose was the language designed?
> Boost.MPI is a library for message passing in high-performance parallel applications. A Boost.MPI program is one or more processes that can communicate either via sending and receiving individual messages (point-to-point communication) or by coordinating as a group (collective communication). Unlike communication in threaded environments or using a shared-memory library, Boost.MPI processes can be spread across many different machines, possibly with different operating systems and underlying architectures.
> Boost.MPI is not a completely new parallel programming library. Rather, it is a C++-friendly interface to the standard Message Passing Interface (MPI), the most popular library interface for high-performance, distributed computing. MPI defines a library interface, available from C, Fortran, and C++, for which there are many MPI implementations. Although there exist C++ bindings for MPI, they offer little functionality over the C bindings. The Boost.MPI library provides an alternative C++ interface to MPI that better supports modern C++ development styles, including complete support for user-defined data types and C++ Standard Library types, arbitrary function objects for collective algorithms, and the use of modern C++ library techniques to maintain maximal efficiency.


2. Syntax

(a) Model implemented:
> Procedural message passing (no sharing).

(b) Class
> Library called from C++.

(c) Not applicable

(d) Readability? Syntactic consistency? Conciseness?
> Readability - boost mpi code is just as readable as any other C++ library code.
> Syntactic consistency - boost mpi library follows modern C++ standards.
> Conciseness - boost mpi produces relatively concise code, one line collective operations for example are quite complex inside.

(e) Common errors
> Boost mpi may take some time to setup.
> Common programming errors include passing messages of incorrect size or passing incorrect messages, which usually results in application termination.


3. Type Philosophy and Data Types

(a) What primitive types does the language support?
> All C++ primitive data types are supported.

(b) How are new types created?
> Just like in any other C++ program by introducing a new class, structure, etc.

(c) What intrinsic operations are defined on user-defined types?
> No additional operations are defined on user-defined types automatically.

(d) How strict is typing? How is type conversion done?
> C++ type conversion is followed.

(e) Can type information be inspected at run-time?
> C++ type information is available at run-time.

(f) What concurrent operations on aggregates (such as lists and arrays)
are intrinsic? What kinds of concurrent aggregate operations can users
define?
> Most communication methods can either take single value or take an array or std::vector of values to perform an operation on. For example, broadcast can either be given a single integer or a pointer to the first integer and the number of integers to broadcast.
> User may specify reduction function for reduce operation.

(g) How are aggregates defined (e.g. by index set, by shape)? Can new
kinds of aggregates (e.g. templates for arrays) be defined?
> Aggregates are either arrays or std::vector's. Templates for aggregates are allowed.

(h) How strict is typing during communication?
> C++ typing scheme is followed during calls to boost mpi functions.

(i) What support is there for communication or manipulation of data of
non-scalar or non-intrinsic types?
> #include <boost/serialization/string.hpp> will add support for communicating strings
> Non-scalar or non-intrinsic types must be serialized in order to be used in communication

(j) Can structural information about aggregates (e.g. number of elements
in a set, dimension of an array) be inspected at run-time?
> Only if you use std::vector.
> Arrays in C++ don't carry additional information.

(k) What built-in support is there for object classes and inheritance? For
polymorphism? For class (as opposed to instance) data?
> Inherited classes can change serialize method, but it must include serialization of base class.

(l) Are functions first-class?
> In general, C++ doesn't support first-class functions.

(m) Are continuations supported? Are they first-class?
> There are non-blocking operations: sendi, recvi.

(n) Is dynamically-allocated memory supported? Is it done automatically?
Is garbage-collection done automatically? Can a program ever create a
dangling pointer? An alias?
> Yes, no, no, yes, no.


4. Operators

(a) What intrinsic operators are provided (brief list)? How strongly do
these type-check their arguments?
> send, sendi
> recv, recvi
> broadcast, gather, reduce
> split
> skeleton, get_content
> Type checking is (C++)-like.

(b) How are new operators defined?
> Not applicable.

(c) Are the precedence rules straightforward?
> Not applicable.

(d) Are side-effects/mutation allowed inside non-call expressions (e.g. C’s
side-effecting i++ construct)?
> Not applicable.

(e) Are their results platform-independent?
> Yes (but depends on whether particular MPI implementation is platform-independent).

(f) Are the results of intrinsic parallel operations reproducible? Are they
platform-independent?
> Yes (but depends on whether particular MPI implementation is platform-independent).

(g) Are heterogeneous operations on aggregates allowed (e.g. every element
of a list of polygons calculates its center of mass)? If so, how are these
described, and how are they implemented?
> Yes, individual processes can perform different tasks.

(h) Which of the operators in Section A.1 are supported?
> regular data motion
> irregular data motion
> message selection by type
> message selection by source
> broadcast and partial broadcast
> split-phase (non-blocking) operations
> data marshalling and unmarshalling of “flat” structures (e.g. arrays of scalars)
> distributed linked structures in systems without physically-shared memory


5. Control Flow

(a) What synchronization is automatically imposed during execution (i.e.
at what points can definite assertions about the state of the running
program be made)?
> Synchronization is done during collective operations, ie broadcast, gather, and reduce.

(b) What control constructs does the language support? Encourage?
> No explicit synchronization functions are available.

(c) How is concurrency expressed? How structured is this mechanism?
> Concurrency is expressed by using multiple processes that can run on different machines.

(d) Can the degree of concurrency be throttled (limited)? Is this done by
the system, or by the user?
> This is done by the user

(e) How are exceptions handled/reported? Is it different for intrinsic operations
and user-defined operations?
> boost mpi errors are translated into exceptions using boost::mpi::exception class.


6. Subroutines, Scope, and Modularization

(a) How efficient is subroutine call compared to in-line execution (i.e. how
expensive are parameter passing and non-local control flow)?
> Not applicable.

(b) How much information about parameters is available (required) inside
called subroutine (e.g. whether object is shared with other processes or
private to executing process)?
> Not applicable.

(c) Are side-effecting procedures allowed?
> Not applicable.

(d) Can generic routines be written (e.g. to work on different types or array
shapes)?
> Not applicable.

(e) Is there a restriction on return values (e.g. can a function return any
object which could be declared)?
> Not applicable.

(f) Are there levels of scope? If multiple levels are permitted, how are references
to stale scopes handled or prevented? Can information for declarations
(such as size) be inherited (dynamically) from higher scopes?
> Not applicable.

(g) What synchronization requirements (if any) are imposed during calls
(i.e. do all processes (in a group?) have to enter/exit the same subroutine
at the same time)?
> Not applicable.

(h) What support is there for modularization? For hiding implementation
details? For sharing such information? For detecting and resolving
name clashes?
> Not applicable.


7. Concurrent Programming Facilities

(a) At what level(s) is the system concurrent?
> Procedural (heterogeneous control parallelism), one execution thread per address space.

(b) What mutual exclusion and synchronization primitives are provided?
> None/hidden from user

(c) At what level(s) is data sharing visible to the programmer?
> No sharing

(d) What is the conceptual granularity of operations?
> Completely asynchronous execution.

(e) How do processes communicate data values? How do they synchronize?
Are non-blocking operations (ones in which buffers might be in a visibly
volatile state) allowed? How are they checked for termination?
> Either peer-to-peer or collective communication.
> Non-blocking operations are allowed. Termination is not checked.

(f) If data are communicated explicitly, how does the receiver choose what
to accept?
> By address or,
> By explicit message tagging or,
> In FIFO order.

(g) How do users specify the number of processors on which to run? Must
this be built into the program, or can it be decided at run-time? Is
there support for executing on heterogeneous processors?
> Determined at start up.

(h) Can the user specify the mapping of processes to processors? If so,
how? How tightly coupled are the mapping of processes to processors,
and the mapping of distributed data structures (if supported)?
> No.

(i) Can the user specify the mapping of data structures to processors? Can
data structures be decomposed or partitioned?
> No.

(j) What support is there for operations on groups of processes, such as:
> broadcast
> multicast

(k) Can arbitrary process groups be created, or must they be structured
in some way (e.g. as logical mesh)? Can operations involving disjoint
groups be executed concurrently (e.g. a broadcast within one group,
and a barrier within another)?
> Arbitrary process groups.
> Yes.


8. Input/Output
(a) Is simple sequential (single-process) I/O simple to write?
> Yes.

(b) Is concurrent I/O supported?
> No.

(c) What types can be read/written intrinsically? E.g. can an entire record
(or array, or list) be read or written in a single operation?
> Not applicable.


9. Access to Routines in Other Languages and to the Hardware

(a) Can subroutines written in other languages be called? Can subroutines
in this language be called from other languages?
> Whatever is possible in C++

(b) Are there ways to access the run-time system directly, or similar lowlevel
facilities?
> Processes are started from command line. They can be terminated easily.

(c) Is there high-level access to physical addresses and devices?
> No.


10. Practical Considerations

(a) Are both batch and interactive processing supported?
> Not applicable.

(b) How reliable and efficient are existing implementations? How fast is it
(i.e. how quickly does code compile and load)? Is it supported (who
fixes it if it breaks)?
> Didn't encounter any bugs in boost mpi itself.
> Compilation time is increased up to a few seconds per files that uses boost mpi.
> Boost is well maintained.

(c) Is there a standard for the system (from some standards organization)?
> Boost mpi follows MPI 1.1 standard.
> Boost developers claim they follow C++ Coding Standards (http://my.safaribooksonline.com/0321113586)

(d) Is the documentation adequate? For whom (CS graduates, computerliterate
users, the general population)? Is training available? Consulting?
> Tutorial and documentation are adequate for developers.

(e) How much code has been written (lines of code or weeks of effort)? By
whom (computer science undergraduates, professional engineers, etc.)?
> Boost is written by professional engineers. Hard to tell how much code has been written specifically for boost mpi.

(f) How long does it take the compiler to generate a “small” executable
(such as "hello, world")?
> Not long.

(g) How large is a “small” executable? How long does it take to load such
an executable?
> Small (depends on compiler).
> Not long to load, however spawning many processes takes a while.


11. Supporting Tools

(a) What is the development environment like? Can code be developed on
a workstation and then moved to a parallel platform?
> Use any development environment of your choice. And yes.

(b) Are there debugging aids, performance analysers, and management
tools?
> Not provided with boost mpi.

(c) How much help do these really provide?
> Not applicable.

